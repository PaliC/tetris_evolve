Metadata-Version: 2.4
Name: tetris-evolve
Version: 0.1.0
Summary: Tetris AI evolution using AlphaEvolve approach - evolving code-based agents
Author: Tetris Evolve Contributors
License: MIT
Project-URL: Homepage, https://github.com/yourusername/tetris_evolve
Project-URL: Documentation, https://github.com/yourusername/tetris_evolve#readme
Project-URL: Repository, https://github.com/yourusername/tetris_evolve
Project-URL: Issues, https://github.com/yourusername/tetris_evolve/issues
Keywords: tetris,evolution,ai,gymnasium,openevolve,alphaevolve
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Games/Entertainment :: Arcade
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.20.0
Requires-Dist: gymnasium>=0.28.0
Requires-Dist: pufferlib>=0.5.0
Requires-Dist: anthropic>=0.18.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: torch>=2.0.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: matplotlib>=3.7.0
Requires-Dist: pandas>=2.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"

# Tetris Evolve: LLM-Powered Evolutionary System

An autonomous evolutionary system that combines **AlphaEvolve** (evolutionary coding) with **Recursive LLMs** (hierarchical decision-making) to evolve programs that play PufferLib environments optimally.

## Overview

Instead of training neural networks or using hardcoded evolution strategies, we use a **Root LLM** as an autonomous evolutionary strategist that:
- Decides which programs to evolve
- Determines what improvements to focus on
- Spawns **Child Recursive LLMs** to generate code mutations
- Decides when to terminate evolution based on convergence

The Root and Child LLMs share a REPL environment for efficient context passing, enabling sophisticated evolutionary strategies without explicit serialization.

## Key Concepts

### AlphaEvolve + Recursive LLMs
- **AlphaEvolve**: Evolutionary framework with LLM-based code generation and automated evaluation
- **Recursive LLMs (RLM)**: Hierarchical LLM decision-making with shared REPL environment
- **Combination**: Root LLM orchestrates evolution, Child RLLMs generate code mutations

### Architecture

**Root LLM (Depth=0)** - Autonomous Orchestrator
- Analyzes program database and performance metrics
- Decides how many programs to advance (dynamic N)
- Crafts custom prompts for Child RLLMs
- Can assign same parent to multiple RLLMs with different focus areas
- Decides when to terminate evolution

**Child RLLMs (Depth=1)** - Code Generators
- Receive custom prompts from Root
- Generate code mutations based on Root's directives
- Share REPL environment with Root for efficient context access

**Shared REPL Environment**
- Root and Children execute in same Python REPL
- Variables, functions, and context are shared
- Eliminates need for explicit serialization

### Hard Resource Limits
- `max_generations`: Hard threshold Root cannot exceed
- `max_child_rllms_per_generation`: Maximum Child RLLMs per generation
- `max_time_minutes`: Maximum execution time

## Components

### Environment Module (`src/tetris_evolve/environment/`)
- `base.py`: `EnvironmentConfig` ABC for environment-agnostic design
- `wrapper.py`: `GenericEnvironmentWrapper` works with any PufferLib environment
- `tetris.py`: `TetrisConfig` implementation
- `atari.py`: `AtariConfig` example for extensibility

### Evaluation Module (`src/tetris_evolve/evaluation/`)
- Player evaluation framework
- Executes programs against environments
- Collects generic + environment-specific metrics

### Database Module (`src/tetris_evolve/database/`)
- In-memory program database
- File-based persistence (no SQLite initially)
- Stores code, metrics, lineage, metadata

### RLM Module (`src/tetris_evolve/rlm/`)
- Core Recursive LLM framework
- Shared REPL environment implementation
- `spawn_rllm(prompt: str)` function

### Root LLM Module (`src/tetris_evolve/root_llm/`)
- Root LLM logic and orchestration
- Functions: `terminate_evolution()`, `modify_parameters()`, `get_historical_trends()`

### Child LLM Module (`src/tetris_evolve/child_llm/`)
- Child RLLM logic
- Code generation based on Root's custom prompts

### Evolution Module (`src/tetris_evolve/evolution/`)
- Top-level evolution loop
- Coordinates Root LLM, Child RLLMs, evaluation, persistence

## Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/tetris_evolve.git
cd tetris_evolve
```

2. Install in development mode:
```bash
pip install -e ".[dev]"
```

3. Set up your Anthropic API key:
```bash
export ANTHROPIC_API_KEY="your-api-key"
```

### Run Evolution

Run Tetris evolution with default configuration:
```bash
tetris-evolve --config configs/tetris.yaml
```

Run with custom parameters:
```bash
tetris-evolve \
  --config configs/tetris.yaml \
  --max-generations 50 \
  --output-dir runs/my_experiment
```

### Adding a New Environment

1. Create a new `EnvironmentConfig` in `src/tetris_evolve/environment/`:

```python
from tetris_evolve.environment.base import EnvironmentConfig

class MyGameConfig(EnvironmentConfig):
    def get_env_id(self) -> str:
        return "MyGame-v0"

    def get_env_kwargs(self) -> Dict[str, Any]:
        return {"render_mode": None}

    # Implement other abstract methods...
```

2. Create a YAML config in `configs/`:
```yaml
environment:
  type: "mygame"
  env_id: "MyGame-v0"
# ... other settings
```

3. Run evolution:
```bash
tetris-evolve --config configs/mygame.yaml
```

## How Evolution Works

### Phase 1: Initial Population
The system generates an initial population of player programs using the Child RLLMs.

### Phase 2: Evaluation
Each program plays multiple episodes in the environment, collecting metrics like:
- Episode rewards
- Steps survived
- Environment-specific metrics (e.g., lines cleared for Tetris)

### Phase 3: Root LLM Decision
The Root LLM analyzes all programs and their performance, then:
1. Decides how many programs N to advance (dynamic, not hardcoded)
2. Selects which programs to evolve
3. Crafts custom prompts for Child RLLMs
4. Can assign multiple RLLMs to same program with different focus areas

Example Root LLM decision:
```python
# Root LLM has access to shared REPL context:
# - current_generation: int
# - program_database: List[Program]
# - current_gen_programs: List[Program]
# - metrics_summary: Dict

# Root analyzes and decides
selected = [prog for prog in current_gen_programs if prog.score > threshold]

# Root crafts custom prompt for Child RLLM
prompt = f"""
Improve this Tetris player (current score: {selected[0].score}):
{selected[0].code}

This program creates too many holes. Focus on:
1. Reducing hole creation
2. Maintaining current lookahead depth
3. Keeping performance above {selected[0].score * 0.95}

Return only the improved Python code.
"""

new_code = spawn_rllm(prompt)
```

### Phase 4: Child RLLM Generation
Child RLLMs receive custom prompts from Root and generate code mutations. They share the REPL environment, so they have access to:
- Parent program code and metrics
- Historical trends
- Shared utility functions

### Phase 5: Termination Decision
The Root LLM decides whether to continue or terminate based on:
- Convergence (no improvement for N generations)
- Hard resource limits (max_generations, max_time)
- Custom termination criteria

## Output Structure

Evolution runs create a timestamped directory with complete history:

```
runs/run_2025-01-23_14-30-00/
├── config.yaml                    # Configuration used
├── evolution_log.jsonl            # Evolution events
├── root_llm_log.jsonl            # Root LLM decisions
├── summary.json                   # Final results
├── generations/
│   ├── gen_000/
│   │   ├── generation_summary.json
│   │   ├── programs/
│   │   │   ├── prog_00001/
│   │   │   │   ├── metadata.json      # Program info
│   │   │   │   ├── code.py            # Player code
│   │   │   │   ├── metrics.json       # Performance metrics
│   │   │   │   ├── parent_info.json   # Lineage
│   │   │   │   └── evaluation_logs/   # Detailed episode logs
│   │   │   └── prog_00002/
│   │   │       └── ...
│   │   └── rllm_logs/
│   │       ├── rllm_00001.json
│   │       └── rllm_00002.json
│   ├── gen_001/
│   │   └── ...
│   └── ...
├── checkpoints/
│   ├── gen_010_checkpoint.pkl
│   └── ...
└── visualizations/
    ├── score_progression.png
    ├── diversity_metrics.png
    └── ...
```

## Configuration

All configuration is done via YAML files. See `configs/tetris.yaml` for a complete example.

### Key Configuration Sections

**Environment:**
```yaml
environment:
  type: "tetris"
  env_id: "tetris-v0"
  env_kwargs:
    render_mode: null
  evaluation:
    num_episodes: 10
    num_workers: 4
```

**Evolution:**
```yaml
evolution:
  max_generations: 100              # HARD LIMIT
  max_child_rllms_per_generation: 10  # HARD LIMIT
  max_time_minutes: 1440
  initial_population_size: 5
```

**LLMs:**
```yaml
llm:
  root:
    model: "claude-sonnet-4"
    temperature: 0.7
  child:
    model: "claude-sonnet-4"
    temperature: 0.8
```

## Development

### Running Tests
```bash
# All tests
pytest

# Unit tests only
pytest -m unit

# Integration tests only
pytest -m integration

# With coverage
pytest --cov=tetris_evolve --cov-report=html
```

### Code Quality
```bash
# Format code
black src/ tests/

# Lint code
ruff check src/ tests/

# Type check
mypy src/
```

## Project Status

### Phase 0: Project Setup ✓
- [x] Directory structure
- [x] Dependencies and configuration
- [x] Build system

### Phase 1: Environment Abstraction (In Progress)
- [ ] `EnvironmentConfig` ABC
- [ ] `GenericEnvironmentWrapper`
- [ ] `TetrisConfig` implementation
- [ ] Tests

### Phase 2: Evaluation Framework
- [ ] Player evaluator
- [ ] Metrics collection
- [ ] Tests

### Phase 3: Program Database
- [ ] In-memory database
- [ ] File-based persistence
- [ ] Tests

### Phase 4: RLM Framework
- [ ] Shared REPL environment
- [ ] `spawn_rllm()` function
- [ ] Tests

### Phase 5: Root & Child LLMs
- [ ] Root LLM orchestration
- [ ] Child RLLM code generation
- [ ] Tests

### Phase 6: Evolution Loop
- [ ] Main evolution loop
- [ ] Logging and visualization
- [ ] End-to-end tests

## Resources

### Papers
- [AlphaEvolve Paper](https://arxiv.org/abs/2506.13131) - Evolutionary coding with LLMs
- [Recursive LLM](https://github.com/alexzhang13/rlm) - Hierarchical LLM decision-making

### Libraries
- [PufferLib](https://github.com/PufferAI/PufferLib) - Fast RL environment wrapper
- [Gymnasium](https://gymnasium.farama.org/) - Standard RL environment API
- [Anthropic API](https://docs.anthropic.com/) - Claude LLM API

### Design
- See `DESIGN.md` for complete architectural details
- Configuration examples in `configs/`

## Contributing

This is an experimental research project. Contributions welcome!

Areas for contribution:
- Additional environment configurations
- Improved Root LLM prompting strategies
- Visualization tools
- Performance optimizations

## License

MIT
