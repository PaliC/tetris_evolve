{
  "trial_id": "trial_3_10",
  "generation": 3,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\n\ndef construct_packing():\n    N = 26\n    \n    # Initialize using a perturbed grid to ensure good distribution\n    # 5x5 grid = 25 circles, plus one extra\n    gx = np.linspace(1/10, 9/10, 5)\n    gy = np.linspace(1/10, 9/10, 5)\n    xx, yy = np.meshgrid(gx, gy)\n    x_init = xx.flatten()\n    y_init = yy.flatten()\n    \n    # Add the 26th circle near the center\n    x_init = np.append(x_init, 0.5)\n    y_init = np.append(y_init, 0.5)\n    \n    # Add random noise to break symmetry\n    np.random.seed(42)\n    x_init += np.random.uniform(-0.05, 0.05, N)\n    y_init += np.random.uniform(-0.05, 0.05, N)\n    \n    # Initial radii guess (slightly conservative)\n    r_init = np.full(N, 0.09)\n    \n    # Parameter vector: [x_1...x_N, y_1...y_N, r_1...r_N]\n    params = np.concatenate([x_init, y_init, r_init])\n    \n    # Precompute indices for pairwise distance calculations\n    idx_i, idx_j = np.triu_indices(N, k=1)\n    \n    def unpack(p):\n        return p[:N], p[N:2*N], p[2*N:]\n    \n    def objective(p):\n        x, y, r = unpack(p)\n        \n        # 1. Boundary Penalties\n        # r <= x, r <= 1-x, r <= y, r <= 1-y\n        # Violations are positive when constraint is broken\n        v_b = (np.maximum(0, r - x)**2 + \n               np.maximum(0, x + r - 1)**2 + \n               np.maximum(0, r - y)**2 + \n               np.maximum(0, y + r - 1)**2)\n        \n        # 2. Overlap Penalties\n        dx = x[idx_i] - x[idx_j]\n        dy = y[idx_i] - y[idx_j]\n        d_sq = dx**2 + dy**2\n        d = np.sqrt(d_sq + 1e-12)\n        r_sum = r[idx_i] + r[idx_j]\n        v_o = np.maximum(0, r_sum - d)**2\n        \n        # 3. Radii Positivity\n        v_r = np.maximum(0, -r)**2\n        \n        # Objective: Maximize sum(r) => Minimize -sum(r) + Weighted Penalties\n        penalty_weight = 1e6\n        return -np.sum(r) + penalty_weight * (np.sum(v_b) + np.sum(v_o) + np.sum(v_r))\n\n    # --- CMA-ES Approximation (Simplified) ---\n    # We use a (mu, lambda)-ES with isotropic covariance adaptation\n    mean = params.copy()\n    sigma = 0.02\n    pop_size = 40\n    mu = pop_size // 4\n    \n    # Weights for recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu+1))\n    weights /= np.sum(weights)\n    \n    for generation in range(80):\n        # Sample candidates\n        candidates = mean + sigma * np.random.randn(pop_size, len(params))\n        \n        # Evaluate\n        scores = np.array([objective(c) for c in candidates])\n        \n        # Selection\n        sorted_indices = np.argsort(scores)\n        best_candidates = candidates[sorted_indices[:mu]]\n        \n        # Recombination\n        new_mean = np.sum(best_candidates * weights[:, None], axis=0)\n        \n        # Step size adaptation (1/5th success rule approximation)\n        # If best offspring is better than previous mean, increase step size\n        if scores[sorted_indices[0]] < objective(mean):\n            sigma *= 1.05\n        else:\n            sigma *= 0.95\n            \n        mean = new_mean\n        sigma *= 0.99 # Annealing\n\n    # --- Refinement with L-BFGS-B ---\n    # Defines bounds for variables to assist the optimizer\n    bounds = [(0, 1)] * (2 * N) + [(0, 0.5)] * N\n    \n    res = minimize(objective, mean, method='L-BFGS-B', bounds=bounds, \n                   options={'maxiter': 2000, 'ftol': 1e-9})\n    \n    final_p = res.x\n    x, y, r = unpack(final_p)\n    \n    # --- Post-Processing: Strict Constraint Enforcement ---\n    # 1. Enforce Boundary Constraints\n    r = np.maximum(0, r)\n    r = np.minimum(r, x)\n    r = np.minimum(r, 1-x)\n    r = np.minimum(r, y)\n    r = np.minimum(r, 1-y)\n    \n    # 2. Enforce Overlap Constraints Iteratively\n    # Shrink radii where overlaps occur until valid\n    for _ in range(500):\n        dx = x[idx_i] - x[idx_j]\n        dy = y[idx_i] - y[idx_j]\n        d = np.sqrt(dx**2 + dy**2)\n        r_sum = r[idx_i] + r[idx_j]\n        overlaps = r_sum - d\n        \n        if np.all(overlaps <= 1e-9):\n            break\n            \n        mask = overlaps > 1e-9\n        scale_factors = np.ones(N)\n        \n        # Calculate necessary scaling for overlapping pairs\n        active_indices = np.where(mask)[0]\n        for k in active_indices:\n            i = idx_i[k]\n            j = idx_j[k]\n            dist = d[k]\n            curr_sum = r[i] + r[j]\n            if curr_sum > 1e-12:\n                ratio = dist / curr_sum\n                scale_factors[i] = min(scale_factors[i], ratio)\n                scale_factors[j] = min(scale_factors[j], ratio)\n        \n        # Apply scaling with a tiny safety margin\n        r *= (scale_factors * 0.999999)\n\n    centers = np.vstack((x, y)).T\n    return centers, r, np.sum(r)\n\ndef run_packing():\n    return construct_packing()",
  "metrics": {
    "valid": true,
    "score": 1.6554994932085956,
    "eval_time": 0.42067718505859375,
    "error": null
  },
  "prompt": "Solve the circle packing problem (26 circles). \n\nStrategy: **CMA-ES Approximation**.\n- Implement a simplified Covariance Matrix Adaptation Evolution Strategy using standard python/numpy.\n- Maintain a mean vector (centers) and step size.\n- Sample candidates, evaluate (radius sum - overlap penalty), update mean toward best candidates.\n- This is powerful for continuous non-convex problems.\n\nREQUIREMENTS:\n1. Output valid Python code ONLY.\n2. Wrap code in a ```python``` block.\n3. NO conversational text before or after.\n4. NO explanations.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef construct_packing():\n    N = 26\n    \n    # Initialize using a perturbed grid to ensure good distribution\n    # 5x5 grid = 25 circles, plus one extra\n    gx = np.linspace(1/10, 9/10, 5)\n    gy = np.linspace(1/10, 9/10, 5)\n    xx, yy = np.meshgrid(gx, gy)\n    x_init = xx.flatten()\n    y_init = yy.flatten()\n    \n    # Add the 26th circle near the center\n    x_init = np.append(x_init, 0.5)\n    y_init = np.append(y_init, 0.5)\n    \n    # Add random noise to break symmetry\n    np.random.seed(42)\n    x_init += np.random.uniform(-0.05, 0.05, N)\n    y_init += np.random.uniform(-0.05, 0.05, N)\n    \n    # Initial radii guess (slightly conservative)\n    r_init = np.full(N, 0.09)\n    \n    # Parameter vector: [x_1...x_N, y_1...y_N, r_1...r_N]\n    params = np.concatenate([x_init, y_init, r_init])\n    \n    # Precompute indices for pairwise distance calculations\n    idx_i, idx_j = np.triu_indices(N, k=1)\n    \n    def unpack(p):\n        return p[:N], p[N:2*N], p[2*N:]\n    \n    def objective(p):\n        x, y, r = unpack(p)\n        \n        # 1. Boundary Penalties\n        # r <= x, r <= 1-x, r <= y, r <= 1-y\n        # Violations are positive when constraint is broken\n        v_b = (np.maximum(0, r - x)**2 + \n               np.maximum(0, x + r - 1)**2 + \n               np.maximum(0, r - y)**2 + \n               np.maximum(0, y + r - 1)**2)\n        \n        # 2. Overlap Penalties\n        dx = x[idx_i] - x[idx_j]\n        dy = y[idx_i] - y[idx_j]\n        d_sq = dx**2 + dy**2\n        d = np.sqrt(d_sq + 1e-12)\n        r_sum = r[idx_i] + r[idx_j]\n        v_o = np.maximum(0, r_sum - d)**2\n        \n        # 3. Radii Positivity\n        v_r = np.maximum(0, -r)**2\n        \n        # Objective: Maximize sum(r) => Minimize -sum(r) + Weighted Penalties\n        penalty_weight = 1e6\n        return -np.sum(r) + penalty_weight * (np.sum(v_b) + np.sum(v_o) + np.sum(v_r))\n\n    # --- CMA-ES Approximation (Simplified) ---\n    # We use a (mu, lambda)-ES with isotropic covariance adaptation\n    mean = params.copy()\n    sigma = 0.02\n    pop_size = 40\n    mu = pop_size // 4\n    \n    # Weights for recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu+1))\n    weights /= np.sum(weights)\n    \n    for generation in range(80):\n        # Sample candidates\n        candidates = mean + sigma * np.random.randn(pop_size, len(params))\n        \n        # Evaluate\n        scores = np.array([objective(c) for c in candidates])\n        \n        # Selection\n        sorted_indices = np.argsort(scores)\n        best_candidates = candidates[sorted_indices[:mu]]\n        \n        # Recombination\n        new_mean = np.sum(best_candidates * weights[:, None], axis=0)\n        \n        # Step size adaptation (1/5th success rule approximation)\n        # If best offspring is better than previous mean, increase step size\n        if scores[sorted_indices[0]] < objective(mean):\n            sigma *= 1.05\n        else:\n            sigma *= 0.95\n            \n        mean = new_mean\n        sigma *= 0.99 # Annealing\n\n    # --- Refinement with L-BFGS-B ---\n    # Defines bounds for variables to assist the optimizer\n    bounds = [(0, 1)] * (2 * N) + [(0, 0.5)] * N\n    \n    res = minimize(objective, mean, method='L-BFGS-B', bounds=bounds, \n                   options={'maxiter': 2000, 'ftol': 1e-9})\n    \n    final_p = res.x\n    x, y, r = unpack(final_p)\n    \n    # --- Post-Processing: Strict Constraint Enforcement ---\n    # 1. Enforce Boundary Constraints\n    r = np.maximum(0, r)\n    r = np.minimum(r, x)\n    r = np.minimum(r, 1-x)\n    r = np.minimum(r, y)\n    r = np.minimum(r, 1-y)\n    \n    # 2. Enforce Overlap Constraints Iteratively\n    # Shrink radii where overlaps occur until valid\n    for _ in range(500):\n        dx = x[idx_i] - x[idx_j]\n        dy = y[idx_i] - y[idx_j]\n        d = np.sqrt(dx**2 + dy**2)\n        r_sum = r[idx_i] + r[idx_j]\n        overlaps = r_sum - d\n        \n        if np.all(overlaps <= 1e-9):\n            break\n            \n        mask = overlaps > 1e-9\n        scale_factors = np.ones(N)\n        \n        # Calculate necessary scaling for overlapping pairs\n        active_indices = np.where(mask)[0]\n        for k in active_indices:\n            i = idx_i[k]\n            j = idx_j[k]\n            dist = d[k]\n            curr_sum = r[i] + r[j]\n            if curr_sum > 1e-12:\n                ratio = dist / curr_sum\n                scale_factors[i] = min(scale_factors[i], ratio)\n                scale_factors[j] = min(scale_factors[j], ratio)\n        \n        # Apply scaling with a tiny safety margin\n        r *= (scale_factors * 0.999999)\n\n    centers = np.vstack((x, y)).T\n    return centers, r, np.sum(r)\n\ndef run_packing():\n    return construct_packing()\n```",
  "reasoning": "",
  "timestamp": "2026-01-08T10:24:56.266800",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 345,
        "candidates_token_count": 1678,
        "thoughts_token_count": 7709
      },
      "response_text_len": 4759
    }
  }
}